{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PS2-5 Kernelizing the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recall the update rule\n",
    "\n",
    "$$\\theta^{(i + 1)} := \\theta^{(i)} + \\alpha \\big( y^{(i + 1)} - h_{\\theta^{(i)}} (\\phi (x^{(i + 1)})) \\big) \\phi (x^{(i + 1)})$$\n",
    "\n",
    "We can easily figure out that $\\theta^{(i)}$ is a linear combination of $\\phi (x^{(1)}), \\dots , \\phi (x^{(i)})$, that is\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta^{(i)} & = \\sum_{j = 1}^{i} \\beta_j \\phi (x^{(j)}) \\\\\n",
    "\\theta^{(0)} & = \\vec{0}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "h_{\\theta^{(i)}} (\\phi (x^{(i + 1)})) & = \\mathrm{sign} ((\\theta^{(i)})^T \\phi (x^{(i + 1)})) \\\\\n",
    "                                      & = \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j \\phi (x^{(j)})^T \\phi (x^{(i + 1)}) \\big) \\\\\n",
    "                                      & = \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j \\langle \\phi (x^{(j)}) , \\phi (x^{(i + 1)}) \\rangle \\big) \\\\\n",
    "                                      & = \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j K(x^{(j)}, x^{(i + 1)}) \\big)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\theta^{(i + 1)} : & = \\theta^{(i)} + \\alpha \\big( y^{(i + 1)} - h_{\\theta^{(i)}} (\\phi (x^{(i + 1)})) \\big) \\phi (x^{(i + 1)}) \\\\\n",
    "                   & = \\sum_{j = 1}^{i} \\beta_j \\phi (x^{(j)}) + \\underbrace{\\alpha ( y^{(i + 1)} - \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j K(x^{(j)}, x^{(i + 1)}) \\big) )}_{\\beta_{i + 1}} \\phi (x^{(i + 1)}) \\\\\n",
    "                   & = \\sum_{j = 1}^{i + 1} \\beta_j \\phi (x^{(j)})\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, the new update rule is:\n",
    "\n",
    "$$\\beta_{i + 1} := \\alpha ( y^{(i + 1)} - \\mathrm{sign} \\big( \\sum_{j = 1}^{i} \\beta_j K(x^{(j)}, x^{(i + 1)}) \\big) )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def initial_state():\n",
    "    \"\"\"Return the initial state for the perceptron.\n",
    "\n",
    "    Returns:\n",
    "        An array of tuples. Each tuple comprises a training example and the corresponding `beta`.\n",
    "    \"\"\"\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(state, kernel, x_i):\n",
    "    \"\"\"Perform a prediction on a given instance x_i given the current state and the kernel.\n",
    "\n",
    "    Args:\n",
    "        state: The state returned from initial_state()\n",
    "        kernel: A binary function that takes two vectors as input and returns the result of a kernel\n",
    "        x_i: A vector containing the features for a single instance\n",
    "\n",
    "    Returns:\n",
    "        Returns the prediction (i.e 0 or 1)\n",
    "    \"\"\"\n",
    "    return sign(sum(beta * kernel(x, x_i) for beta, x in state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_state(state, kernel, learning_rate, x_i, y_i):\n",
    "    \"\"\"Updates the state of the perceptron.\n",
    "\n",
    "    Args:\n",
    "        state: The state returned from initial_state()\n",
    "        kernel: A binary function that takes two vectors as input and returns the result of a kernel\n",
    "        learning_rate: The learning rate for the update\n",
    "        x_i: A vector containing the features for a single instance\n",
    "        y_i: A 0 or 1 indicating the label for a single instance\n",
    "    \"\"\"\n",
    "    beta_i = learning_rate * (y_i - sign(sum(beta * kernel(x, x_i) for beta, x in state)))\n",
    "    state.append((beta_i, x_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copy over the remaining code from `p05_percept.py` (no change on logic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import problem_set_2.src.util as util\n",
    "\n",
    "\n",
    "def sign(a):\n",
    "    \"\"\"Gets the sign of a scalar input.\"\"\"\n",
    "    if a >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def dot_kernel(a, b):\n",
    "    \"\"\"An implementation of a dot product kernel.\n",
    "\n",
    "    Args:\n",
    "        a: A vector\n",
    "        b: A vector\n",
    "    \"\"\"\n",
    "    return np.dot(a, b)\n",
    "\n",
    "\n",
    "def rbf_kernel(a, b, sigma=1):\n",
    "    \"\"\"An implementation of the radial basis function kernel.\n",
    "\n",
    "    Args:\n",
    "        a: A vector\n",
    "        b: A vector\n",
    "        sigma: The radius of the kernel\n",
    "    \"\"\"\n",
    "    distance = (a - b).dot(a - b)\n",
    "    scaled_distance = -distance / (2 * (sigma) ** 2)\n",
    "    return math.exp(scaled_distance)\n",
    "\n",
    "\n",
    "def train_perceptron(kernel_name, kernel, learning_rate):\n",
    "    \"\"\"Train a perceptron with the given kernel.\n",
    "\n",
    "    This function trains a perceptron with a given kernel and then\n",
    "    uses that perceptron to make predictions.\n",
    "    The output predictions are saved to src/output/p05_{kernel_name}_predictions.txt.\n",
    "    The output plots are saved to src/output_{kernel_name}_output.pdf.\n",
    "\n",
    "    Args:\n",
    "        kernel_name: The name of the kernel.\n",
    "        kernel: The kernel function.\n",
    "        learning_rate: The learning rate for training.\n",
    "    \"\"\"\n",
    "    train_x, train_y = util.load_csv('data/ds5_train.csv')\n",
    "\n",
    "    state = initial_state()\n",
    "\n",
    "    for x_i, y_i in zip(train_x, train_y):\n",
    "        update_state(state, kernel, learning_rate, x_i, y_i)\n",
    "\n",
    "    test_x, test_y = util.load_csv('data/ds5_train.csv')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    util.plot_contour(lambda a: predict(state, kernel, a))\n",
    "    util.plot_points(test_x, test_y)\n",
    "\n",
    "\n",
    "train_perceptron('dot', dot_kernel, 0.5)\n",
    "train_perceptron('rbf', rbf_kernel, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the product kernel, there is no feature mapping for the input attributes. In other words, the model is still linear\n",
    "after applying the product kernel. As a consequence, it performs badly on the data that are not linearly separable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "cs229"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
