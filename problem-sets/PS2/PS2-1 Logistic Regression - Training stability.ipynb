{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PS2-1 Convexity of Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Output:\n",
    "\n",
    "```\n",
    "==== Training model on data set A ====\n",
    "Finished 10000 iterations\n",
    "Finished 20000 iterations\n",
    "Finished 30000 iterations\n",
    "Converged in 30395 iterations\n",
    "==== Training model on data set B ====\n",
    "Finished 10000 iterations\n",
    "Finished 20000 iterations\n",
    "Finished 30000 iterations\n",
    "Finished 40000 iterations\n",
    "Finished 50000 iterations\n",
    "Finished 60000 iterations\n",
    "Finished 70000 iterations\n",
    "Finished 80000 iterations\n",
    "Finished 90000 iterations\n",
    "Finished 100000 iterations\n",
    "...\n",
    "...\n",
    "```\n",
    "\n",
    "The algorithm converges on dataset A, but not on B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import problem_set_2.src.util as util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train_a, y_train_a = util.load_csv('data/ds1_a.csv', add_intercept=True)\n",
    "x_train_b, y_train_b = util.load_csv('data/ds1_b.csv', add_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot(x, y, title):\n",
    "    plt.figure()\n",
    "    plt.plot(x[y == 1, -2], x[y == 1, -1], 'bx', linewidth=2)\n",
    "    plt.plot(x[y == -1, -2], x[y == -1, -1], 'go', linewidth=2)\n",
    "    plt.suptitle(title, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot dataset A and B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "plot(x_train_a, y_train_a, 'Dataset A')\n",
    "plot(x_train_b, y_train_b, 'Dataset B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At first glance, the only difference between the two datasets seems to be that dataset B is linearly separable, whereas dataset A is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the code:\n",
    "\n",
    "```python\n",
    "def calc_grad(X, Y, theta):\n",
    "    \"\"\"Compute the gradient of the loss with respect to theta.\"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    margins = Y * X.dot(theta)\n",
    "    probs = 1. / (1 + np.exp(margins))\n",
    "    grad = -(1./m) * (X.T.dot(probs * Y))\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def logistic_regression(X, Y):\n",
    "    \"\"\"Train a logistic regression model.\"\"\"\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    learning_rate = 10\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        prev_theta = theta\n",
    "        grad = calc_grad(X, Y, theta)\n",
    "        theta = theta - learning_rate * grad\n",
    "        if i % 10000 == 0:\n",
    "            print('Finished %d iterations' % i)\n",
    "        if np.linalg.norm(prev_theta - theta) < 1e-15:\n",
    "            print('Converged in %d iterations' % i)\n",
    "            break\n",
    "    return\n",
    "```\n",
    "\n",
    "we can see that the gradient of the cost function is\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = - \\frac{1}{m} \\sum_{i = 1}^{m} \\frac{y^{(i)} x^{(i)}}{1 + \\exp (y^{(i)} \\theta^T x^{(i)})}$$\n",
    "\n",
    "which means that the gradient descent algorithm is trying to minimize\n",
    "\n",
    "$$\\ell (\\theta) = - \\frac{1}{m} \\sum_{i = 1}^{m} \\log \\frac{1}{1 + \\exp (-y^{(i)} \\theta^T x^{(i)})}$$\n",
    "\n",
    "If a dataset is completely linearly separable, i.e. $\\forall i \\in \\{1, \\dots, m \\}, \\ y^{(i)} \\theta^T x^{(i)} > 0$,\n",
    "then, by multiplying a larger positive scalar, there will always be a new $\\theta$ that makes $\\ell (\\theta)$ even smaller,\n",
    "which prevents the algorithm from converging. However, if the dataset is not linearly separable, $\\theta$ cannot be generated\n",
    "in such way while minimizing $\\ell (\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "No. A different fixed learning rate is only a scalar for $\\nabla_\\theta J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. By decreasing the learning rate over time, the algorithm will eventually find some $\\theta$ whose change is small enough\n",
    "to meet the stop criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. Linear scaling of the input features can be treated as a scalar applied to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### iv.\n",
    "\n",
    "Yes. An L2 regularization term can keep $\\theta$ from arbitrarily scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Yes. Adding zero-mean Gaussian noise to the training data or labels helps so long as the dataset is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "No. SVM with hinge loss is not vulnerable to linearly separable datasets.\n",
    "\n",
    "If the dataset is linearly separable, the hinge loss will be minimized to 0, and the algorithm then stops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229",
   "language": "python",
   "name": "cs229"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
