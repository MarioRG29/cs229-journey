{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\cs229-journey\\cs229-journey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42945aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import problem_sets.PS1.src.util as util\n",
    "\n",
    "from problem_sets.PS1.src.linear_model import LinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adf405",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_training_set_path = 'data/ds1_train.csv'\n",
    "ds1_valid_set_path = 'data/ds1_valid.csv'\n",
    "ds2_training_set_path = 'data/ds2_train.csv'\n",
    "ds2_valid_set_path = 'data/ds2_valid.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37576a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds1_training_set_path, add_intercept=True)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path, add_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c758a92b",
   "metadata": {},
   "source": [
    "We have a look to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ffaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[:, 1], x_train[:, 2], 'x')\n",
    "plt.plot(x_valid[:, 1], x_valid[:, 2], '.')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend([\"Train data\", \"Validation Data\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ca7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[:, 1][y_train == 1], x_train[:, 2][y_train == 1], 'x')\n",
    "plt.plot(x_train[:, 1][y_train == 0], x_train[:, 2][y_train == 0], 'x')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend([\"Features with label y = 1\", \"Features with label y = 0\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6e81d",
   "metadata": {},
   "source": [
    "b) Follow the instructions in src/p01b logreg.py to train a\n",
    "logistic regression classifier using Newton’s Method. Starting with θ = 0, run Newton’s\n",
    "Method until the updates to θ are small: Specifically, train until the first iteration k such\n",
    "that $‖θ_k −θ_{k−1}‖_1 <\\epsilon$, where $ \\epsilon = 1 ×10^{-5}$. Make sure to write your model’s predictions to\n",
    "the file specified in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be4122",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(LinearModel):\n",
    "    \"\"\"Logistic regression with Newton's Method as the solver.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = LogisticRegression()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run Newton's Method to minimize J(theta) for logistic regression.\n",
    "\n",
    "        :param x: Training example inputs. Shape (m, n).\n",
    "        :param y: Training example labels. Shape (m,).\n",
    "        \"\"\"\n",
    "        def sigmoid(theta):\n",
    "            return 1/(1+np.exp(-np.dot(x, theta)))\n",
    "        \n",
    "        def gradient_loss(theta):\n",
    "            return -np.dot(x.T, (y - sigmoid(theta)))/m\n",
    "        \n",
    "        def hessian(theta):\n",
    "            h_theta_x = np.reshape(sigmoid(theta), (-1, 1))\n",
    "            return 1 / m * np.dot(x.T, h_theta_x * (1 - h_theta_x) * x)\n",
    "        \n",
    "        def newton_step(H, gradient, theta):\n",
    "            return theta - np.dot(np.linalg.inv(H), gradient)\n",
    "        \n",
    "        m, n = x.shape\n",
    "\n",
    "        self.theta = np.zeros(n)\n",
    "        while True:\n",
    "            theta = np.copy(self.theta)\n",
    "            gradient = gradient_loss(theta)\n",
    "            H = hessian(theta)\n",
    "            self.theta = newton_step(H, gradient, theta)\n",
    "            if np.linalg.norm(self.theta - theta) < self.eps:\n",
    "                break\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        :param x: Inputs of shape (m, n).\n",
    "        :return:  Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "        probs = 1/(1+np.exp(-np.dot(x, self.theta)))\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f179ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(eps=1e-5)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot(x_train, y_train, theta=model.theta, correction=1)\n",
    "plt.legend([\"Class y = 1\", \"Class y = 0\", \"Decision Boundary\"])\n",
    "plt.grid()\n",
    "print(\"Optimal theta is: \", model.theta)\n",
    "print(f\"The accuracy on training set is: {np.mean((model.predict(x_train)>0.5) == y_train):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff85430",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot(x_valid, y_valid, model.theta)\n",
    "plt.legend([\"Class y = 1\", \"Class y = 0\", \"Decision Boundary\"])\n",
    "plt.grid()\n",
    "print(f\"The accuracy on validation set is: {np.mean((model.predict(x_valid) > 0.5) == y_valid):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5c742",
   "metadata": {},
   "source": [
    "e)  In src/p01e_gda.py, fill in the code to calculate $\\phi$, $\\mu_0$, $\\mu_1$ and $\\Sigma$, use these parameters to derive $\\theta$, and use the resulting GDA model to make\n",
    " predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c35c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the data without x_0 = 1\n",
    "x_train, y_train = util.load_dataset(ds1_training_set_path, add_intercept=False)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path, add_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d348be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDA(LinearModel):\n",
    "    \"\"\"Gaussian Discriminant Analysis.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = GDA()\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fit a GDA model to training set given by x and y.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (m, n).\n",
    "            y: Training example labels. Shape (m,).\n",
    "\n",
    "        Returns:\n",
    "            theta: GDA model parameters.\n",
    "        \"\"\"\n",
    "        m, n = x.shape\n",
    "        phi = np.sum(y) / m\n",
    "        mu_0 = np.sum(x[y == 0], axis= 0)/np.sum(y == 0)\n",
    "        mu_1 = np.sum(x[y == 1], axis = 0)/np.sum(y)\n",
    "        y_reshaped = y.reshape(m, 1)\n",
    "        mu_x = y_reshaped * mu_1 + (1 - y_reshaped) * mu_0\n",
    "        x_centered = x - mu_x\n",
    "        sigma = np.dot(x_centered.T, x_centered)/m\n",
    "        inv_sigma = np.linalg.inv(sigma)\n",
    "        #From exercise c)\n",
    "        self.theta = np.dot(inv_sigma, (mu_1 - mu_0))\n",
    "        self.theta_0 = -1/2 * (mu_1 @ inv_sigma @ mu_1.T - mu_0 @ inv_sigma @ mu_0.T) - np.log((1-phi)/phi)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given new inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "\n",
    "        Returns:\n",
    "            Outputs of shape (m,).\n",
    "        \"\"\"\n",
    "        probs = 1/(1+np.exp(-(x@self.theta + self.theta_0)))\n",
    "        return probs\n",
    "        # *** END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde0c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gda = GDA()\n",
    "model_gda.fit(x = x_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b11c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot(x_train, y_train, theta=np.insert(model_gda.theta, 0, model_gda.theta_0))\n",
    "plt.legend([\"Class y = 1\", \"Class y = 0\", \"Decision Boundary\"])\n",
    "plt.grid()\n",
    "print(\"Optimal theta is: \", model.theta)\n",
    "print(f\"The accuracy on training set is: {np.mean((model_gda.predict(x_train)>0.5) == y_train):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18233223",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot(x_valid, y_valid, theta=np.insert(model_gda.theta, 0, model_gda.theta_0))\n",
    "plt.legend([\"Class y = 1\", \"Class y = 0\", \"Decision Boundary\"])\n",
    "plt.grid()\n",
    "print(\"Optimal theta is: \", model.theta)\n",
    "print(f\"The accuracy on validation set is: {np.mean((model_gda.predict(x_valid) > 0.5) == y_valid):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8a3e3",
   "metadata": {},
   "source": [
    "f) For Dataset 1, create a plot of the training data with $x_1$ on the horizontal axis, and\n",
    " $x_2$ on the vertical axis. To visualize the two classes, use a di erent symbol for examples $x^{(i)}$\n",
    " with $y^{(i)} = 0$ than for those with $y^{(i)} = 1$. On the same gure, plot the decision boundary\n",
    " found by logistic regression in part (b). Make an identical plot with the decision boundary\n",
    " found by GDA in part (e)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f274c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, y, theta_1, theta_2, save_path=None, correction=1.0):\n",
    "    \"\"\"Plot dataset and fitted logistic regression parameters and GDA parameters.\n",
    "    Args:\n",
    "        x: Matrix of training examples, one per row.\n",
    "        y: Vector of labels in {0, 1}.\n",
    "        theta_1: Vector of parameters for logistic regression model.\n",
    "        theta_2: Vector of parameters for GDA model.\n",
    "        save_path: Path to save the plot.\n",
    "        correction: Correction factor to apply (Problem 2(e) only).\n",
    "    \"\"\"\n",
    "    # Plot dataset\n",
    "    plt.figure()\n",
    "    plt.plot(x[y == 1, -2], x[y == 1, -1], 'bx', linewidth=2, label = \"Class y = 1\")\n",
    "    plt.plot(x[y == 0, -2], x[y == 0, -1], 'go', linewidth=2, label = \"Class y = 0\")\n",
    "\n",
    "    # Plot decision boundary (found by solving for theta^T x = 0)\n",
    "    margin1 = (max(x[:, -2]) - min(x[:, -2]))*0.2\n",
    "    margin2 = (max(x[:, -1]) - min(x[:, -1]))*0.2\n",
    "    x1 = np.arange(min(x[:, -2])-margin1, max(x[:, -2])+margin1, 0.01)\n",
    "    x2 = -(theta_1[0] / theta_1[2] * correction + theta_1[1] / theta_1[2] * x1)\n",
    "    x3 = -(theta_2[0] / theta_2[2] * correction + theta_2[1] / theta_2[2] * x1)\n",
    "    plt.plot(x1, x2, c='red', linewidth=2, label = \"Logistic Regression Decision Boundary\")\n",
    "    plt.plot(x1, x3, c='black', linewidth=2, label = \"GDA Decision Boundary\")\n",
    "    plt.xlim(x[:, -2].min()-margin1, x[:, -2].max()+margin1)\n",
    "    plt.ylim(x[:, -1].min()-margin2, x[:, -1].max()+margin2)\n",
    "\n",
    "    # Add labels and save to disk\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50bddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_valid, y_valid, theta_1=model.theta, theta_2=np.insert(model_gda.theta, 0, model_gda.theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16926f",
   "metadata": {},
   "source": [
    "We see that the decision boundary of the GDA Model is worts that the decision boundary of the LR model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfd082f",
   "metadata": {},
   "source": [
    "(g) [5 points] Repeat the steps in part (f) for Dataset 2. On which dataset does GDA seem to\n",
    "perform worse than logistic regression? Why might this be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds2_training_set_path, add_intercept=False)\n",
    "x_valid, y_valid = util.load_dataset(ds2_valid_set_path, add_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[:, 0], x_train[:, 1], '.')\n",
    "plt.plot(x_valid[:, 0], x_valid[:, 1], 'x')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend([\"Train data\", \"Validation Data\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train[:, 0][y_train == 1], x_train[:, 1][y_train == 1], 'x')\n",
    "plt.plot(x_train[:, 0][y_train == 0], x_train[:, 1][y_train == 0], 'x')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend([\"Class y = 1\", \"Class y = 0\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde60b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we train LogisticRegression\n",
    "x_train_intercep = util.add_intercept(x_train)\n",
    "x_valid_intercep = util.add_intercept(x_valid)\n",
    "model.fit(x_train_intercep, y_train)\n",
    "model.predict(x_valid_intercep)\n",
    "\n",
    "error_train = np.mean((model.predict(x_train_intercep)>0.5) == y_train)\n",
    "test_error = np.mean((model.predict(x_valid_intercep)>0.5) == y_valid)\n",
    "print(f\"Training accuracy: {error_train:.2%}\")\n",
    "print(f\"Training accuracy: {test_error:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we train GDA\n",
    "x_train, y_train = util.load_dataset(ds2_training_set_path, add_intercept=False)\n",
    "x_valid, y_valid = util.load_dataset(ds2_valid_set_path, add_intercept=False)\n",
    "model_gda.fit(x_train, y_train)\n",
    "model_gda.predict(x_valid)\n",
    "\n",
    "error_train = np.mean((model_gda.predict(x_train)>0.5) == y_train)\n",
    "test_error = np.mean((model_gda.predict(x_valid)>0.5) == y_valid)\n",
    "print(f\"Training accuracy: {error_train:.2%}\")\n",
    "print(f\"Training accuracy: {test_error:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770456d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_valid, y_valid, theta_1=model.theta, theta_2=np.insert(model_gda.theta, 0, model_gda.theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67483e0",
   "metadata": {},
   "source": [
    "In this case the GDA performs as good as the LR. This is because in this case the data follows a Gaussian distribution, which was not the case with the dataset 1 as we can see in the plot of x2 vs x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c565f",
   "metadata": {},
   "source": [
    "h) For the dataset where GDA performed worse in parts (f) and (g),\n",
    " can you nd a transformation of the $x^{(i)}$ s such that GDA performs signicantly better?\n",
    " What is this transformation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98436ace",
   "metadata": {},
   "source": [
    "Looking the second plot of the b) exercise we see that $x_2 > 0$ and also that have significant high values we can apply a log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = util.load_dataset(ds1_training_set_path, add_intercept=False)\n",
    "x_valid, y_valid = util.load_dataset(ds1_valid_set_path, add_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a6cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed = np.stack((x_train[:, 0], np.log(x_train[:,1])),axis=1)\n",
    "x_valid_transformed = np.stack((x_valid[:, 0], np.log(x_valid[:,1])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aba751",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train_transformed[:, 0], x_train_transformed[:, 1], 'x')\n",
    "plt.plot(x_valid_transformed[:, 0], x_valid_transformed[:, 1], '.')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend([\"Train data\", \"Validation Data\"])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we train LR\n",
    "x_train_transformed_intercep = util.add_intercept(x_train_transformed)\n",
    "x_valid_transformed_intercep = util.add_intercept(x_valid_transformed)\n",
    "model.fit(x_train_transformed_intercep, y_train)\n",
    "\n",
    "error_train = np.mean((model.predict(x_train_transformed_intercep)>0.5) == y_train)\n",
    "test_error = np.mean((model.predict(x_valid_transformed_intercep)>0.5) == y_valid)\n",
    "print(f\"Training accuracy on logistic regression: {error_train:.2%}\")\n",
    "print(f\"Validation accuracy on logistic regression: {test_error:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we train GDA\n",
    "model_gda.fit(x_train_transformed, y_train)\n",
    "\n",
    "error_train = np.mean((model_gda.predict(x_train_transformed)>0.5) == y_train)\n",
    "test_error = np.mean((model_gda.predict(x_valid_transformed)>0.5) == y_valid)\n",
    "print(f\"Training accuracy on GDA: {error_train:.2%}\")\n",
    "print(f\"Validation accuracy on GDA: {test_error:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7294d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_train_transformed, y_train, theta_1= model.theta, theta_2=np.insert(model_gda.theta, 0, model_gda.theta_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa70e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x_valid_transformed, y_valid, theta_1= model.theta, theta_2=np.insert(model_gda.theta, 0, model_gda.theta_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e07a1",
   "metadata": {},
   "source": [
    "In this case we see that both algorithms behave the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
